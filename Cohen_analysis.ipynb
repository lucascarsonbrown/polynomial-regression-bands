{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b269512",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq100_tickers = [\n",
    "    \"AAPL\", \"ABNB\", \"ADBE\", \"ADI\", \"ADP\", \"ADSK\", \"AEP\", \"AMD\", \"AMAT\", \"AMGN\",\n",
    "    \"AMZN\", \"APP\", \"ARM\", \"ASML\", \"AVGO\", \"AXON\", \"AZN\", \"BIIB\", \"BKNG\", \"BKR\",\n",
    "    \"CCEP\", \"CDNS\", \"CDW\", \"CEG\", \"CHTR\", \"CMCSA\", \"COST\", \"CPRT\", \"CRWD\", \"CSCO\",\n",
    "    \"CSGP\", \"CSX\", \"CTAS\", \"CTSH\", \"DASH\", \"DDOG\", \"DXCM\", \"EA\", \"EXC\", \"FANG\",\n",
    "    \"FAST\", \"FTNT\", \"GEHC\", \"GFS\", \"GILD\", \"GOOG\", \"GOOGL\", \"HON\", \"IDXX\", \"INTC\",\n",
    "    \"INTU\", \"ISRG\", \"KDP\", \"KHC\", \"KLAC\", \"LIN\", \"LRCX\", \"LULU\", \"MAR\", \"MCHP\",\n",
    "    \"MDLZ\", \"MELI\", \"META\", \"MNST\", \"PEP\", \"PLTR\", \"PYPL\", \"QCOM\", \"REGN\", \"ROP\",\n",
    "    \"ROST\", \"SBUX\", \"SHOP\", \"SNPS\", \"TEAM\", \"TMUS\", \"TSLA\", \"TTD\", \"TTWO\", \"TXN\",\n",
    "    \"VRSK\", \"VRTX\", \"WBD\", \"WDAY\", \"XEL\", \"ZS\"\n",
    "]\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31044abe",
   "metadata": {},
   "source": [
    "Now, I am curious about Cohen's statement regarding that:\n",
    "- A degree 4 polynomial is best\n",
    "- A 60 day window is best\n",
    "- A band width of 2 is best\n",
    "\n",
    "I am curious to find out if this is correct, by testing every possibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ba77110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid Search\n",
    "from utils import run_full_analysis, calculate_metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define parameter ranges\n",
    "degrees = range(1, 10)\n",
    "windows = range(20, 121, 10)\n",
    "bandwidths = np.arange(0.5, 4.1, 0.1)\n",
    "\n",
    "# Directory for stock data\n",
    "data_dir = \"nasdaq100_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "g2quqzh2jum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 10/3564 (0.3%) - Calculated: 10, Cached: 0\n",
      "Progress: 20/3564 (0.6%) - Calculated: 20, Cached: 0\n",
      "Progress: 30/3564 (0.8%) - Calculated: 30, Cached: 0\n",
      "Progress: 40/3564 (1.1%) - Calculated: 40, Cached: 0\n",
      "Progress: 50/3564 (1.4%) - Calculated: 50, Cached: 0\n",
      "Progress: 60/3564 (1.7%) - Calculated: 60, Cached: 0\n",
      "Progress: 70/3564 (2.0%) - Calculated: 70, Cached: 0\n",
      "Progress: 80/3564 (2.2%) - Calculated: 80, Cached: 0\n",
      "Progress: 90/3564 (2.5%) - Calculated: 90, Cached: 0\n",
      "Progress: 100/3564 (2.8%) - Calculated: 100, Cached: 0\n",
      "Progress: 110/3564 (3.1%) - Calculated: 110, Cached: 0\n",
      "Progress: 120/3564 (3.4%) - Calculated: 120, Cached: 0\n",
      "Progress: 130/3564 (3.6%) - Calculated: 130, Cached: 0\n",
      "Progress: 140/3564 (3.9%) - Calculated: 140, Cached: 0\n",
      "Progress: 150/3564 (4.2%) - Calculated: 150, Cached: 0\n",
      "Progress: 160/3564 (4.5%) - Calculated: 160, Cached: 0\n",
      "Progress: 170/3564 (4.8%) - Calculated: 170, Cached: 0\n",
      "Progress: 180/3564 (5.1%) - Calculated: 180, Cached: 0\n",
      "Progress: 190/3564 (5.3%) - Calculated: 190, Cached: 0\n",
      "Progress: 200/3564 (5.6%) - Calculated: 200, Cached: 0\n",
      "Progress: 210/3564 (5.9%) - Calculated: 210, Cached: 0\n",
      "Progress: 220/3564 (6.2%) - Calculated: 220, Cached: 0\n",
      "Progress: 230/3564 (6.5%) - Calculated: 230, Cached: 0\n",
      "Progress: 240/3564 (6.7%) - Calculated: 240, Cached: 0\n",
      "Progress: 250/3564 (7.0%) - Calculated: 250, Cached: 0\n",
      "Progress: 260/3564 (7.3%) - Calculated: 260, Cached: 0\n",
      "Progress: 270/3564 (7.6%) - Calculated: 270, Cached: 0\n",
      "Progress: 280/3564 (7.9%) - Calculated: 280, Cached: 0\n",
      "Progress: 290/3564 (8.1%) - Calculated: 290, Cached: 0\n",
      "Progress: 300/3564 (8.4%) - Calculated: 300, Cached: 0\n",
      "Progress: 310/3564 (8.7%) - Calculated: 310, Cached: 0\n",
      "Progress: 320/3564 (9.0%) - Calculated: 320, Cached: 0\n",
      "Progress: 330/3564 (9.3%) - Calculated: 330, Cached: 0\n",
      "Progress: 340/3564 (9.5%) - Calculated: 340, Cached: 0\n",
      "Progress: 350/3564 (9.8%) - Calculated: 350, Cached: 0\n",
      "Progress: 360/3564 (10.1%) - Calculated: 360, Cached: 0\n",
      "Progress: 370/3564 (10.4%) - Calculated: 370, Cached: 0\n",
      "Progress: 380/3564 (10.7%) - Calculated: 380, Cached: 0\n",
      "Progress: 390/3564 (10.9%) - Calculated: 390, Cached: 0\n",
      "Progress: 400/3564 (11.2%) - Calculated: 400, Cached: 0\n",
      "Progress: 410/3564 (11.5%) - Calculated: 410, Cached: 0\n",
      "Progress: 420/3564 (11.8%) - Calculated: 420, Cached: 0\n",
      "Progress: 430/3564 (12.1%) - Calculated: 430, Cached: 0\n",
      "Progress: 440/3564 (12.3%) - Calculated: 440, Cached: 0\n",
      "Progress: 450/3564 (12.6%) - Calculated: 450, Cached: 0\n",
      "Progress: 460/3564 (12.9%) - Calculated: 460, Cached: 0\n",
      "Progress: 470/3564 (13.2%) - Calculated: 470, Cached: 0\n",
      "Progress: 480/3564 (13.5%) - Calculated: 480, Cached: 0\n",
      "Progress: 490/3564 (13.7%) - Calculated: 490, Cached: 0\n",
      "Progress: 500/3564 (14.0%) - Calculated: 500, Cached: 0\n",
      "Progress: 510/3564 (14.3%) - Calculated: 510, Cached: 0\n",
      "Progress: 520/3564 (14.6%) - Calculated: 520, Cached: 0\n",
      "Progress: 530/3564 (14.9%) - Calculated: 530, Cached: 0\n",
      "Progress: 540/3564 (15.2%) - Calculated: 540, Cached: 0\n",
      "Progress: 550/3564 (15.4%) - Calculated: 550, Cached: 0\n",
      "Progress: 560/3564 (15.7%) - Calculated: 560, Cached: 0\n",
      "Progress: 570/3564 (16.0%) - Calculated: 570, Cached: 0\n",
      "Progress: 580/3564 (16.3%) - Calculated: 580, Cached: 0\n",
      "Progress: 590/3564 (16.6%) - Calculated: 590, Cached: 0\n",
      "Progress: 600/3564 (16.8%) - Calculated: 600, Cached: 0\n",
      "Progress: 610/3564 (17.1%) - Calculated: 610, Cached: 0\n",
      "Progress: 620/3564 (17.4%) - Calculated: 620, Cached: 0\n",
      "Progress: 630/3564 (17.7%) - Calculated: 630, Cached: 0\n",
      "Progress: 640/3564 (18.0%) - Calculated: 640, Cached: 0\n",
      "Progress: 650/3564 (18.2%) - Calculated: 650, Cached: 0\n",
      "Progress: 660/3564 (18.5%) - Calculated: 660, Cached: 0\n",
      "Progress: 670/3564 (18.8%) - Calculated: 670, Cached: 0\n",
      "Progress: 680/3564 (19.1%) - Calculated: 680, Cached: 0\n",
      "Progress: 690/3564 (19.4%) - Calculated: 690, Cached: 0\n",
      "Progress: 700/3564 (19.6%) - Calculated: 700, Cached: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m df = pd.read_csv(csv_path, index_col=\u001b[32m0\u001b[39m, parse_dates=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m df_analyzed = \u001b[43mrun_full_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbandwidth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[32m     67\u001b[39m metrics = calculate_metrics(df_analyzed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/PersonalProjects/PolynomialRegression/utils.py:80\u001b[39m, in \u001b[36mrun_full_analysis\u001b[39m\u001b[34m(df, degree, window, k, investment)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_full_analysis\u001b[39m(df, degree=\u001b[32m4\u001b[39m, window=\u001b[32m60\u001b[39m, k=\u001b[32m2\u001b[39m, investment=\u001b[32m10000\u001b[39m):\n\u001b[32m     79\u001b[39m     df = df.copy()\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     df = \u001b[43mcompute_polynomial_bands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     df = assign_signals(df)\n\u001b[32m     82\u001b[39m     df = assign_positions(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/PersonalProjects/PolynomialRegression/utils.py:34\u001b[39m, in \u001b[36mcompute_polynomial_bands\u001b[39m\u001b[34m(df, degree, window, k)\u001b[39m\n\u001b[32m     31\u001b[39m center = poly(window)\n\u001b[32m     33\u001b[39m idx = df.index[i]\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPoly_Center\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m = center\n\u001b[32m     35\u001b[39m df.at[idx, \u001b[33m'\u001b[39m\u001b[33mUpper_Band\u001b[39m\u001b[33m'\u001b[39m] = center + k * sigma\n\u001b[32m     36\u001b[39m df.at[idx, \u001b[33m'\u001b[39m\u001b[33mLower_Band\u001b[39m\u001b[33m'\u001b[39m] = center - k * sigma\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/core/indexing.py:2603\u001b[39m, in \u001b[36m_AtIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   2600\u001b[39m     \u001b[38;5;28mself\u001b[39m.obj.loc[key] = value\n\u001b[32m   2601\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2603\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/core/indexing.py:2543\u001b[39m, in \u001b[36m_ScalarAccessIndexer.__setitem__\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) != \u001b[38;5;28mself\u001b[39m.ndim:\n\u001b[32m   2541\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot enough indexers for scalar access (setting)!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2543\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/core/frame.py:4572\u001b[39m, in \u001b[36mDataFrame._set_value\u001b[39m\u001b[34m(self, index, col, value, takeable)\u001b[39m\n\u001b[32m   4570\u001b[39m     iindex = cast(\u001b[38;5;28mint\u001b[39m, index)\n\u001b[32m   4571\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4572\u001b[39m     icol = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4573\u001b[39m     iindex = \u001b[38;5;28mself\u001b[39m.index.get_loc(index)\n\u001b[32m   4574\u001b[39m \u001b[38;5;28mself\u001b[39m._mgr.column_setitem(icol, iindex, value, inplace_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3810\u001b[39m casted_key = \u001b[38;5;28mself\u001b[39m._maybe_cast_indexer(key)\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run grid search and save all data (with resume capability)\n",
    "results = []\n",
    "\n",
    "# Create main output directory\n",
    "output_dir = \"Analysis_Data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "total_iterations = len(list(degrees)) * len(list(windows)) * len(bandwidths)\n",
    "current_iteration = 0\n",
    "skipped_count = 0\n",
    "calculated_count = 0\n",
    "\n",
    "for degree in degrees:\n",
    "    for window in windows:\n",
    "        for bandwidth in bandwidths:\n",
    "            current_iteration += 1\n",
    "            \n",
    "            # Create directory for this parameter combination\n",
    "            param_dir = os.path.join(\n",
    "                output_dir, \n",
    "                f\"degree_{degree}\",\n",
    "                f\"window_{window}\",\n",
    "                f\"bandwidth_{bandwidth:.1f}\"\n",
    "            )\n",
    "            os.makedirs(param_dir, exist_ok=True)\n",
    "            \n",
    "            # Check if this combination has already been calculated\n",
    "            summary_path = os.path.join(param_dir, \"summary.csv\")\n",
    "            all_metrics_path = os.path.join(param_dir, \"all_metrics.csv\")\n",
    "            \n",
    "            if os.path.exists(summary_path) and os.path.exists(all_metrics_path):\n",
    "                # Load existing results\n",
    "                try:\n",
    "                    summary_df = pd.read_csv(summary_path)\n",
    "                    results.append({\n",
    "                        'degree': summary_df.iloc[0]['degree'],\n",
    "                        'window': summary_df.iloc[0]['window'],\n",
    "                        'bandwidth': summary_df.iloc[0]['bandwidth'],\n",
    "                        'avg_return': summary_df.iloc[0]['avg_return'],\n",
    "                        'num_stocks': summary_df.iloc[0]['num_stocks']\n",
    "                    })\n",
    "                    skipped_count += 1\n",
    "                    \n",
    "                    if current_iteration % 100 == 0:\n",
    "                        print(f\"Progress: {current_iteration}/{total_iterations} ({100*current_iteration/total_iterations:.1f}%) - Loaded cached result\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load cached result for {param_dir}, recalculating...\")\n",
    "            \n",
    "            # Calculate this combination (not yet cached)\n",
    "            ticker_returns = []\n",
    "            all_ticker_metrics = []\n",
    "            \n",
    "            for ticker in nasdaq100_tickers:\n",
    "                try:\n",
    "                    # Load stock data\n",
    "                    csv_path = os.path.join(data_dir, f\"{ticker}.csv\")\n",
    "                    if not os.path.exists(csv_path):\n",
    "                        continue\n",
    "                    \n",
    "                    df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "                    \n",
    "                    # Run analysis\n",
    "                    df_analyzed = run_full_analysis(df, degree=degree, window=window, k=bandwidth)\n",
    "                    \n",
    "                    # Calculate metrics\n",
    "                    metrics = calculate_metrics(df_analyzed)\n",
    "                    metrics['ticker'] = ticker  # Add ticker to metrics\n",
    "                    \n",
    "                    # Store metrics for this ticker\n",
    "                    all_ticker_metrics.append(metrics)\n",
    "                    \n",
    "                    # Store the strategy return percentage\n",
    "                    ticker_returns.append(metrics['strategy_return_pct'])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Skip tickers with errors\n",
    "                    continue\n",
    "            \n",
    "            # Calculate average return across all tickers\n",
    "            if ticker_returns:\n",
    "                avg_return = np.mean(ticker_returns)\n",
    "                \n",
    "                results.append({\n",
    "                    'degree': degree,\n",
    "                    'window': window,\n",
    "                    'bandwidth': bandwidth,\n",
    "                    'avg_return': avg_return,\n",
    "                    'num_stocks': len(ticker_returns)\n",
    "                })\n",
    "                \n",
    "                # Save all metrics in one CSV\n",
    "                all_metrics_df = pd.DataFrame(all_ticker_metrics)\n",
    "                all_metrics_df.to_csv(all_metrics_path, index=False)\n",
    "                \n",
    "                # Save summary for this parameter combination\n",
    "                pd.DataFrame([{\n",
    "                    'degree': degree,\n",
    "                    'window': window,\n",
    "                    'bandwidth': bandwidth,\n",
    "                    'avg_return': avg_return,\n",
    "                    'num_stocks': len(ticker_returns),\n",
    "                    'min_return': min(ticker_returns),\n",
    "                    'max_return': max(ticker_returns),\n",
    "                    'median_return': np.median(ticker_returns),\n",
    "                    'std_return': np.std(ticker_returns)\n",
    "                }]).to_csv(summary_path, index=False)\n",
    "                \n",
    "                calculated_count += 1\n",
    "            \n",
    "            if current_iteration % 10 == 0 and calculated_count > 0:\n",
    "                print(f\"Progress: {current_iteration}/{total_iterations} ({100*current_iteration/total_iterations:.1f}%) - Calculated: {calculated_count}, Cached: {skipped_count}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"EXPERIMENT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total combinations: {len(results_df)}\")\n",
    "print(f\"  Newly calculated: {calculated_count}\")\n",
    "print(f\"  Loaded from cache: {skipped_count}\")\n",
    "print(f\"\\nBest 10 parameter combinations by average return:\")\n",
    "print(results_df.nlargest(10, 'avg_return')[['degree', 'window', 'bandwidth', 'avg_return']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xnxhl01blz7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D surface plots for each degree\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"Analysis_Data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Determine grid layout\n",
    "n_degrees = len(list(degrees))\n",
    "n_cols = 3\n",
    "n_rows = (n_degrees + n_cols - 1) // n_cols\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5 * n_rows))\n",
    "\n",
    "for idx, degree in enumerate(degrees, 1):\n",
    "    # Filter results for this degree\n",
    "    degree_data = results_df[results_df['degree'] == degree]\n",
    "    \n",
    "    if len(degree_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Extract data\n",
    "    x = degree_data['bandwidth'].values\n",
    "    y = degree_data['window'].values\n",
    "    z = degree_data['avg_return'].values\n",
    "    \n",
    "    # Create meshgrid for interpolation\n",
    "    xi = np.linspace(x.min(), x.max(), 50)\n",
    "    yi = np.linspace(y.min(), y.max(), 50)\n",
    "    xi, yi = np.meshgrid(xi, yi)\n",
    "    \n",
    "    # Interpolate z values\n",
    "    zi = griddata((x, y), z, (xi, yi), method='cubic')\n",
    "    \n",
    "    # Create 3D subplot\n",
    "    ax = fig.add_subplot(n_rows, n_cols, idx, projection='3d')\n",
    "    \n",
    "    # Create surface plot\n",
    "    surf = ax.plot_surface(xi, yi, zi, cmap='viridis', alpha=0.8, edgecolor='none')\n",
    "    \n",
    "    # Also plot the actual data points\n",
    "    ax.scatter(x, y, z, c='red', marker='o', s=20, alpha=0.6)\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Bandwidth (k)', fontsize=10)\n",
    "    ax.set_ylabel('Window (days)', fontsize=10)\n",
    "    ax.set_zlabel('Avg Return (%)', fontsize=10)\n",
    "    ax.set_title(f'Degree {degree}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "    \n",
    "    # Find best parameters for this degree\n",
    "    best_idx = degree_data['avg_return'].idxmax()\n",
    "    best_params = degree_data.loc[best_idx]\n",
    "    \n",
    "    # Annotate best point\n",
    "    ax.text2D(0.05, 0.95, \n",
    "              f\"Best: W={best_params['window']:.0f}, K={best_params['bandwidth']:.1f}\\nReturn={best_params['avg_return']:.2f}%\",\n",
    "              transform=ax.transAxes, fontsize=9, verticalalignment='top',\n",
    "              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join(output_dir, 'parameter_optimization_3d.png')\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"3D plots saved as '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rtz5qkw6s5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Analysis and Comparison to Cohen's Parameters\n",
    "print(\"=\"*80)\n",
    "print(\"PARAMETER OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall best parameters\n",
    "best_overall = results_df.loc[results_df['avg_return'].idxmax()]\n",
    "print(f\"\\nOverall Best Parameters:\")\n",
    "print(f\"  Degree: {best_overall['degree']:.0f}\")\n",
    "print(f\"  Window: {best_overall['window']:.0f} days\")\n",
    "print(f\"  Bandwidth: {best_overall['bandwidth']:.1f}\")\n",
    "print(f\"  Average Return: {best_overall['avg_return']:.2f}%\")\n",
    "print(f\"  Tested on {best_overall['num_stocks']:.0f} stocks\")\n",
    "\n",
    "# Cohen's recommended parameters\n",
    "cohen_params = results_df[\n",
    "    (results_df['degree'] == 4) & \n",
    "    (results_df['window'] == 60) & \n",
    "    (results_df['bandwidth'] == 2.0)\n",
    "]\n",
    "\n",
    "print(f\"\\nCohen's Recommended Parameters (Degree=4, Window=60, Bandwidth=2):\")\n",
    "if len(cohen_params) > 0:\n",
    "    cohen_return = cohen_params.iloc[0]['avg_return']\n",
    "    print(f\"  Average Return: {cohen_return:.2f}%\")\n",
    "    print(f\"  Rank: {(results_df['avg_return'] > cohen_return).sum() + 1} out of {len(results_df)}\")\n",
    "    percentile = 100 * (results_df['avg_return'] <= cohen_return).sum() / len(results_df)\n",
    "    print(f\"  Percentile: {percentile:.1f}%\")\n",
    "else:\n",
    "    print(\"  Not tested in this grid search\")\n",
    "\n",
    "# Best parameters by degree\n",
    "print(f\"\\nBest Parameters by Degree:\")\n",
    "print(\"-\" * 80)\n",
    "for degree in degrees:\n",
    "    degree_data = results_df[results_df['degree'] == degree]\n",
    "    if len(degree_data) > 0:\n",
    "        best = degree_data.loc[degree_data['avg_return'].idxmax()]\n",
    "        print(f\"  Degree {degree}: Window={best['window']:.0f}, Bandwidth={best['bandwidth']:.1f}, Return={best['avg_return']:.2f}%\")\n",
    "\n",
    "# Create a heatmap for degree=4 (Cohen's degree)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating heatmap for Degree 4 (Cohen's recommended degree)...\")\n",
    "\n",
    "degree_4_data = results_df[results_df['degree'] == 4].copy()\n",
    "\n",
    "# Pivot the data for heatmap\n",
    "heatmap_data = degree_4_data.pivot(index='window', columns='bandwidth', values='avg_return')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "im = plt.imshow(heatmap_data.values, cmap='RdYlGn', aspect='auto')\n",
    "\n",
    "# Set ticks and labels\n",
    "plt.xticks(range(len(heatmap_data.columns)), heatmap_data.columns)\n",
    "plt.yticks(range(len(heatmap_data.index)), heatmap_data.index)\n",
    "plt.xlabel('Bandwidth (k)', fontsize=12)\n",
    "plt.ylabel('Window (days)', fontsize=12)\n",
    "plt.title('Average Return Heatmap for Degree 4 Polynomial', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Average Return (%)', rotation=270, labelpad=20)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(heatmap_data.index)):\n",
    "    for j in range(len(heatmap_data.columns)):\n",
    "        value = heatmap_data.iloc[i, j]\n",
    "        if not np.isnan(value):\n",
    "            color = 'white' if value < heatmap_data.values[~np.isnan(heatmap_data.values)].mean() else 'black'\n",
    "            plt.text(j, i, f'{value:.1f}', ha='center', va='center', color=color, fontsize=8)\n",
    "\n",
    "# Highlight Cohen's parameters (window=60, bandwidth=2)\n",
    "if 60 in heatmap_data.index and 2.0 in heatmap_data.columns:\n",
    "    cohen_i = list(heatmap_data.index).index(60)\n",
    "    cohen_j = list(heatmap_data.columns).index(2.0)\n",
    "    plt.plot(cohen_j, cohen_i, 'b*', markersize=20, markeredgewidth=2, markeredgecolor='blue', \n",
    "             markerfacecolor='none', label=\"Cohen's params\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "heatmap_path = os.path.join(output_dir, 'degree_4_heatmap.png')\n",
    "plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Heatmap saved as '{heatmap_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sv34xxyr5zm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV for further analysis\n",
    "csv_path = os.path.join(output_dir, 'parameter_optimization_results.csv')\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Results saved to '{csv_path}'\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal parameter combinations tested: {len(results_df)}\")\n",
    "print(f\"\\nAverage Return Statistics:\")\n",
    "print(f\"  Mean: {results_df['avg_return'].mean():.2f}%\")\n",
    "print(f\"  Median: {results_df['avg_return'].median():.2f}%\")\n",
    "print(f\"  Std Dev: {results_df['avg_return'].std():.2f}%\")\n",
    "print(f\"  Min: {results_df['avg_return'].min():.2f}%\")\n",
    "print(f\"  Max: {results_df['avg_return'].max():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sr8a19lj9te",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive index file\n",
    "print(\"\\nCreating index file...\")\n",
    "\n",
    "index_data = []\n",
    "\n",
    "for degree in degrees:\n",
    "    for window in windows:\n",
    "        for bandwidth in bandwidths:\n",
    "            param_dir = os.path.join(\n",
    "                output_dir, \n",
    "                f\"degree_{degree}\",\n",
    "                f\"window_{window}\",\n",
    "                f\"bandwidth_{bandwidth:.1f}\"\n",
    "            )\n",
    "            \n",
    "            summary_path = os.path.join(param_dir, \"summary.csv\")\n",
    "            \n",
    "            if os.path.exists(summary_path):\n",
    "                summary_df = pd.read_csv(summary_path)\n",
    "                summary_dict = summary_df.iloc[0].to_dict()\n",
    "                summary_dict['directory'] = param_dir\n",
    "                index_data.append(summary_dict)\n",
    "\n",
    "index_df = pd.DataFrame(index_data)\n",
    "index_path = os.path.join(output_dir, 'master_index.csv')\n",
    "index_df.to_csv(index_path, index=False)\n",
    "\n",
    "print(f\"Master index saved to '{index_path}'\")\n",
    "print(f\"\\nTotal data files created: {len(nasdaq100_tickers) * len(index_df) * 2}\")  # 2 files per ticker (analysis + metrics)\n",
    "print(f\"Total directories created: {len(index_df)}\")\n",
    "print(f\"Total summary files: {len(index_df)}\")\n",
    "print(f\"\\nAll analysis data is organized in '{output_dir}/' directory structure:\")\n",
    "print(\"  Analysis_Data/\")\n",
    "print(\"    ├── degree_X/\")\n",
    "print(\"    │   ├── window_Y/\")\n",
    "print(\"    │   │   ├── bandwidth_Z/\")\n",
    "print(\"    │   │   │   ├── TICKER_analysis.csv (full data with bands, signals, positions)\")\n",
    "print(\"    │   │   │   ├── TICKER_metrics.csv (calculated metrics)\")\n",
    "print(\"    │   │   │   └── summary.csv (aggregated stats for this parameter combo)\")\n",
    "print(\"    ├── master_index.csv (searchable index of all combinations)\")\n",
    "print(\"    ├── parameter_optimization_results.csv\")\n",
    "print(\"    ├── parameter_optimization_3d.png\")\n",
    "print(\"    └── degree_4_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647tuivcqw",
   "metadata": {},
   "source": [
    "## How to Access Saved Data\n",
    "\n",
    "All analysis results are now saved in the `Analysis_Data/` directory. You can access any specific parameter combination without re-running the entire experiment.\n",
    "\n",
    "### Example: Load results for specific parameters\n",
    "\n",
    "```python\n",
    "# Example: Load analysis for AAPL with degree=4, window=60, bandwidth=2.0\n",
    "degree, window, bandwidth = 4, 60, 2.0\n",
    "ticker = \"AAPL\"\n",
    "\n",
    "# Construct path\n",
    "param_dir = os.path.join(\"Analysis_Data\", f\"degree_{degree}\", f\"window_{window}\", f\"bandwidth_{bandwidth:.1f}\")\n",
    "\n",
    "# Load full analysis data\n",
    "aapl_analysis = pd.read_csv(os.path.join(param_dir, f\"{ticker}_analysis.csv\"), index_col=0, parse_dates=True)\n",
    "\n",
    "# Load metrics\n",
    "aapl_metrics = pd.read_csv(os.path.join(param_dir, f\"{ticker}_metrics.csv\"))\n",
    "\n",
    "# Load summary for this parameter combination\n",
    "combo_summary = pd.read_csv(os.path.join(param_dir, \"summary.csv\"))\n",
    "```\n",
    "\n",
    "### Use the master index to find best combinations\n",
    "\n",
    "```python\n",
    "# Load master index\n",
    "index = pd.read_csv(\"Analysis_Data/master_index.csv\")\n",
    "\n",
    "# Find best parameter combinations\n",
    "top_10 = index.nlargest(10, 'avg_return')\n",
    "\n",
    "# Find all combinations with degree=4\n",
    "degree_4_results = index[index['degree'] == 4]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
